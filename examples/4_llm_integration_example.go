package main

import (
	"context"
	"fmt"
	"log"
	"os"
	"strings"

	"github.com/cloudwego/eino-ext/components/model/openai"
	"github.com/cloudwego/eino/compose"
	"github.com/cloudwego/eino/schema"
	"github.com/joho/godotenv"
)

// ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Graph ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ LLM ‡∏à‡∏£‡∏¥‡∏á
func main() {
	// Load environment
	err := godotenv.Load()
	if err != nil {
		log.Fatal("Error loading .env file")
	}

	ctx := context.Background()
	apiKey := os.Getenv("OPENROUTER_API_KEY")
	if apiKey == "" {
		fmt.Println("Please set OPENROUTER_API_KEY environment variable")
		return
	}

	// ‡∏™‡∏£‡πâ‡∏≤‡∏á model configuration
	config := &openai.ChatModelConfig{
		APIKey:  apiKey,
		BaseURL: "https://openrouter.ai/api/v1",
		Model:   "openai/gpt-3.5-turbo",
	}

	model, err := openai.NewChatModel(ctx, config)
	if err != nil {
		fmt.Printf("Error creating model: %v\n", err)
		return
	}

	// === ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á LLM Pipeline Graph ===
	fmt.Println("=== LLM Pipeline Graph ===")
	runLLMPipeline(ctx, model)

	// === ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á Multi-Step LLM Processing ===
	fmt.Println("\n=== Multi-Step LLM Processing ===")
	runMultiStepLLM(ctx, model)
}

// LLM Pipeline Graph
func runLLMPipeline(ctx context.Context, model *openai.ChatModel) {
	graph := compose.NewGraph[string, string]()

	// Input preprocessor
	preprocessor := compose.InvokableLambda(func(ctx context.Context, input string) ([]*schema.Message, error) {
		// ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° messages ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM
		messages := []*schema.Message{
			schema.SystemMessage("‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI assistant ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"),
			schema.UserMessage(input),
		}
		fmt.Printf("üîß Preprocessor: Prepared %d messages for LLM\n", len(messages))
		return messages, nil
	})

	// LLM processor - ‡πÉ‡∏ä‡πâ model ‡∏à‡∏£‡∏¥‡∏á
	llmProcessor := compose.InvokableLambda(func(ctx context.Context, messages []*schema.Message) (string, error) {
		fmt.Printf("ü§ñ LLM: Processing request...\n")
		
		response, err := model.Generate(ctx, messages)
		if err != nil {
			return "", fmt.Errorf("LLM error: %w", err)
		}
		
		result := response.Content
		fmt.Printf("ü§ñ LLM Response: %s\n", result)
		return result, nil
	})

	// Postprocessor
	postprocessor := compose.InvokableLambda(func(ctx context.Context, input string) (string, error) {
		// ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° metadata
		processed := fmt.Sprintf("‚ú® AI Response: %s\n\nüìù Note: This response was generated by AI", input)
		fmt.Printf("üéØ Postprocessor: Added metadata and formatting\n")
		return processed, nil
	})

	// ‡πÄ‡∏û‡∏¥‡πà‡∏° nodes
	graph.AddLambdaNode("preprocessor", preprocessor)
	graph.AddLambdaNode("llm", llmProcessor)
	graph.AddLambdaNode("postprocessor", postprocessor)

	// ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏° edges
	graph.AddEdge(compose.START, "preprocessor")
	graph.AddEdge("preprocessor", "llm")
	graph.AddEdge("llm", "postprocessor")
	graph.AddEdge("postprocessor", compose.END)

	// Compile ‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö
	runnable, err := graph.Compile(ctx)
	if err != nil {
		fmt.Printf("Error compiling LLM pipeline: %v\n", err)
		return
	}

	testQuestions := []string{
		"‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ Eino Graph ‡πÉ‡∏´‡πâ‡∏ü‡∏±‡∏á‡∏´‡∏ô‡πà‡∏≠‡∏¢",
		"‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á parallel processing ‡πÉ‡∏ô Go",
	}

	for i, question := range testQuestions {
		fmt.Printf("\n--- Question %d ---\n", i+1)
		fmt.Printf("Input: %s\n", question)

		result, err := runnable.Invoke(ctx, question)
		if err != nil {
			fmt.Printf("Error: %v\n", err)
			continue
		}

		fmt.Printf("\n%s\n", result)
	}
}

// Multi-Step LLM Processing
func runMultiStepLLM(ctx context.Context, model *openai.ChatModel) {
	graph := compose.NewGraph[string, string]()

	// Step 1: Analyze user intent
	intentAnalyzer := compose.InvokableLambda(func(ctx context.Context, input string) (string, error) {
		messages := []*schema.Message{
			schema.SystemMessage("‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå intent ‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÄ‡∏õ‡πá‡∏ô: QUESTION, REQUEST, COMMAND, CASUAL"),
			schema.UserMessage(fmt.Sprintf("Input: %s\n\nIntent:", input)),
		}

		response, err := model.Generate(ctx, messages)
		if err != nil {
			return "", err
		}

		intent := strings.TrimSpace(response.Content)
		result := fmt.Sprintf("%s|%s", intent, input)
		fmt.Printf("üéØ Intent Analyzer: '%s' -> Intent: %s\n", input, intent)
		return result, nil
	})

	// Step 2: Context-aware response generator
	responseGenerator := compose.InvokableLambda(func(ctx context.Context, input string) (string, error) {
		parts := strings.SplitN(input, "|", 2)
		if len(parts) != 2 {
			return input, nil
		}

		intent := parts[0]
		userInput := parts[1]

		// ‡∏™‡∏£‡πâ‡∏≤‡∏á context-aware prompt ‡∏ï‡∏≤‡∏° intent
		var systemPrompt string
		switch {
		case strings.Contains(intent, "QUESTION"):
			systemPrompt = "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏£‡∏π‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á"
		case strings.Contains(intent, "REQUEST"):
			systemPrompt = "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠ ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå"
		case strings.Contains(intent, "COMMAND"):
			systemPrompt = "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÅ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"
		default:
			systemPrompt = "‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô AI assistant ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏´‡∏•‡∏∑‡∏≠"
		}

		messages := []*schema.Message{
			schema.SystemMessage(systemPrompt),
			schema.UserMessage(userInput),
		}

		fmt.Printf("ü§ñ Response Generator: Using %s strategy\n", intent)
		response, err := model.Generate(ctx, messages)
		if err != nil {
			return "", err
		}

		result := fmt.Sprintf("[%s] %s", intent, response.Content)
		return result, nil
	})

	// Step 3: Quality checker
	qualityChecker := compose.InvokableLambda(func(ctx context.Context, input string) (string, error) {
		// ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á response
		if len(input) < 50 {
			enhanced := input + "\n\nüí° Tip: For more detailed information, please ask more specific questions."
			fmt.Printf("üîç Quality Checker: Enhanced short response\n")
			return enhanced, nil
		}

		fmt.Printf("üîç Quality Checker: Response quality is good\n")
		return input, nil
	})

	// ‡πÄ‡∏û‡∏¥‡πà‡∏° nodes
	graph.AddLambdaNode("intent_analyzer", intentAnalyzer)
	graph.AddLambdaNode("response_generator", responseGenerator)
	graph.AddLambdaNode("quality_checker", qualityChecker)

	// ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏° edges
	graph.AddEdge(compose.START, "intent_analyzer")
	graph.AddEdge("intent_analyzer", "response_generator")
	graph.AddEdge("response_generator", "quality_checker")
	graph.AddEdge("quality_checker", compose.END)

	// Compile ‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö
	runnable, err := graph.Compile(ctx)
	if err != nil {
		fmt.Printf("Error compiling multi-step LLM: %v\n", err)
		return
	}

	testInputs := []string{
		"Go ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?",
		"‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô function ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö sort array ‡πÑ‡∏î‡πâ‡πÑ‡∏´‡∏°",
		"‡∏™‡∏£‡πâ‡∏≤‡∏á Docker container",
		"‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ",
	}

	for i, input := range testInputs {
		fmt.Printf("\n--- Test %d ---\n", i+1)
		fmt.Printf("Input: %s\n", input)

		result, err := runnable.Invoke(ctx, input)
		if err != nil {
			fmt.Printf("Error: %v\n", err)
			continue
		}

		fmt.Printf("\nResult:\n%s\n", result)
		fmt.Println(strings.Repeat("-", 80))
	}
}